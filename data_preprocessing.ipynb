{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for all the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Goals\n",
    "\n",
    "\n",
    "- **COCO** dataset:\n",
    "    1. Transform the folder structure from [images, annotations] to [train, validation]\n",
    "    2. Transform the instances_train2017.json and instances_val2017.json to text files.\n",
    "        - Bounding Boxes \n",
    "        - Class code\n",
    "        - Segmentation data (mask data with format [(x_i, y_i), (x_(i+1), y_(i+1), ... )])\n",
    "    3. Resize images and annotations\n",
    "    4. Compute mean and standard deviation for the data using 2500 images as samples\n",
    "\n",
    "\n",
    "- **Vis-Drone** dataset:\n",
    "    1. Extract the bounding boxes from format [x_min, y_min, width, height] to [x_min, y_min, x_max, y_max]\n",
    "    2. Resize images and annotations\n",
    "    3. Produce the segmentation data from the bounding box data: [(x_min, y_min), (x_min, y_max), (x_max, y_min), (x_max, y_max)]\n",
    "    4. Compute mean and standard deviation for the data\n",
    "\n",
    "\n",
    "- **UAV-SOD Drone** dataset:\n",
    "    1. Extract the bounding box data and the class codes from XML files and create the text file equivalent\n",
    "    2. Resize images and annotations\n",
    "    3. Produce the segmentation data from the bounding box data: [(x_min, y_min), (x_min, y_max), (x_max, y_min), (x_max, y_max)]\n",
    "    4. Compute mean and standard deviation for the data\n",
    "\n",
    "\n",
    "- City Scapes dataset:\n",
    "    1. Transform the folder structure from [images, annotations] to [train, validation]\n",
    "    2. Resize images and annotations\n",
    "    3. Produce the segmentation data from the bounding box data: [(x_min, y_min), (x_min, y_max), (x_max, y_min), (x_max, y_max)]\n",
    "    4. Compute mean and standard deviation for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..')) \n",
    "import warnings\n",
    "import src.data_preprocessing as preprocessing\n",
    "\n",
    " \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Import base data paths\n",
    "COCO_DATA_PATH = \"data/coco2017/\"\n",
    "VIS_DATA_PATH = \"data/vis_drone_data/\"\n",
    "SOD_DATA_PATH = \"data/uav_sod_data/\"\n",
    "CITY_DATA_PATH = \"city-scapes-data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO2017 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.93s)\n",
      "creating index...\n",
      "index created!\n",
      "Success: Every image file has a matching annotation file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 118287/118287 [1:01:06<00:00, 32.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "Success: Every image file has a matching annotation file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 5000/5000 [02:33<00:00, 32.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the train, test and validation paths\n",
    "train_path      = os.path.join(COCO_DATA_PATH, \"train\")\n",
    "validation_path = os.path.join(COCO_DATA_PATH, \"validation\")\n",
    "\n",
    "coco_paths = [train_path, validation_path]\n",
    "\n",
    "# Fix the annotations format, resize the images\n",
    "for path in coco_paths:\n",
    "    images_path      = os.path.join(path, \"images\")\n",
    "    annotations_path = os.path.join(path, \"annotations\")\n",
    "    \n",
    "    # Annotations and image transformations\n",
    "    preprocessing.convert_coco_annotations(path)\n",
    "    preprocessing.verify_annotations(path)\n",
    "    preprocessing.resize_data(path)\n",
    "    \n",
    "# Get the mean and standard deviation for the COCO training set \n",
    "preprocessing.compute_mean_std(os.path.join(train_path, \"images\") , \"coco_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vis-Drone Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating the right annotations format: 100%|██████████| 6471/6471 [00:01<00:00, 3413.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files edited successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 6471/6471 [07:01<00:00, 15.35it/s]\n",
      "Creating the right annotations format: 100%|██████████| 548/548 [00:00<00:00, 3051.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files edited successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 548/548 [00:28<00:00, 19.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the train, test and validation paths\n",
    "train_path      = os.path.join(VIS_DATA_PATH, \"train\")\n",
    "validation_path = os.path.join(VIS_DATA_PATH, \"validation\")\n",
    "\n",
    "vis_paths = [train_path, validation_path]\n",
    "\n",
    "\n",
    "# Fix the annotations format, resize the images\n",
    "for path in vis_paths:\n",
    "    images_path      = os.path.join(path, \"images\")\n",
    "    annotations_path = os.path.join(path, \"annotations\")\n",
    "    \n",
    "    # Annotations and image transformations\n",
    "    preprocessing.extract_annotation_values(annotations_path)\n",
    "    preprocessing.resize_data(path)\n",
    "    \n",
    "    \n",
    "# Get the mean and standard deviation for the Vis-Drone training set \n",
    "preprocessing.compute_mean_std(os.path.join(train_path, \"images\") , \"vis_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UAV-SOD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting XML to TXT: 100%|██████████| 717/717 [00:00<00:00, 1529.41it/s]\n",
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 717/717 [00:51<00:00, 13.85it/s]\n",
      "Converting XML to TXT: 100%|██████████| 43/43 [00:00<00:00, 1451.32it/s]\n",
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 43/43 [00:03<00:00, 14.31it/s]\n",
      "Converting XML to TXT: 100%|██████████| 84/84 [00:00<00:00, 1423.54it/s]\n",
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 84/84 [00:06<00:00, 13.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the train, test and validation paths\n",
    "train_path      = os.path.join(SOD_DATA_PATH, \"train\")\n",
    "test_path       = os.path.join(SOD_DATA_PATH, \"test\"  )\n",
    "validation_path = os.path.join(SOD_DATA_PATH, \"validation\")\n",
    "\n",
    "uav_paths = [train_path, test_path, validation_path]\n",
    "\n",
    "\n",
    "# Fix the annotations format, resize the images\n",
    "for path in uav_paths:\n",
    "    images_path      = os.path.join(path, \"images\")\n",
    "    annotations_path = os.path.join(path, \"annotations\")\n",
    "    \n",
    "    # Annotation and image transformations\n",
    "    preprocessing.xml_to_txt(annotations_path)\n",
    "    preprocessing.resize_data(path)\n",
    "    \n",
    "\n",
    "# Get the mean and standard deviation for the UAV training set \n",
    "preprocessing.compute_mean_std(os.path.join(train_path, \"images\") , \"uav_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CityScapes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the process by re-organizing the folder structure for the annotations\n",
    "images_path      = \"data/city_scapes_images/leftImg8bit\"\n",
    "annotations_path = \"data/city_scapes_annotations/gtFine\"\n",
    "preprocessing.reorganize_cityscapes(images_path, annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting JSON to Text files: 100%|██████████| 2975/2975 [00:04<00:00, 632.46it/s]\n",
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 2975/2975 [07:27<00:00,  6.64it/s]\n",
      "Converting JSON to Text files: 100%|██████████| 1525/1525 [00:00<00:00, 5681.99it/s]\n",
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 1525/1525 [03:41<00:00,  6.90it/s]\n",
      "Converting JSON to Text files: 100%|██████████| 500/500 [00:00<00:00, 584.38it/s]\n",
      "Resizing the images, annotations and segmentation data to target size (600,600): 100%|██████████| 500/500 [01:19<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the train, test and validation paths\n",
    "train_path      = os.path.join(CITY_DATA_PATH, \"train\")\n",
    "test_path       = os.path.join(CITY_DATA_PATH, \"test\"  )\n",
    "validation_path = os.path.join(CITY_DATA_PATH, \"validation\")\n",
    "\n",
    "city_paths = [train_path, test_path, validation_path]\n",
    "\n",
    "# Fix the annotations format, resize the images\n",
    "for path in city_paths:\n",
    "    images_path      = os.path.join(path, \"images\")\n",
    "    annotations_path = os.path.join(path, \"annotations\")\n",
    "    \n",
    "    # Annotation and image transformations\n",
    "    preprocessing.json_to_text(annotations_path, annotations_path)\n",
    "    preprocessing.resize_data(path)\n",
    "    \n",
    "# Rename the image and annotation files in order to make it easier for the training\n",
    "preprocessing.compute_mean_std(os.path.join(train_path, \"images\"), \"city_scapes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
