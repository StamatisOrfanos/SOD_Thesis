\chapter{Methodology}

In our exploration of various object detection architectures, it became clear that single-stage detector models, while notable for their efficiency, did not 
meet the accuracy demands required for small object detection. Given the challenges associated with detecting smaller objects, we considered multi-stage models
as a more viable solution, because these models are known for their enhanced precision and adeptness at handling intricate detection tasks. Through our research, 
a potential model combination emerged as a great candidate. The Feature Pyramid Network family of models excels in multi-scale representation essential for 
resolving objects of varying sizes, while the Vision Transformers family provides an exceptional ability to grasp global contextual dependencies within images
through the attention modules. 

By integrating these models, as a backbone and a detector we found that we can leverage the complementary strengths of both frameworks, thus creating a 
robust multi-stage model specifically made to achieve the best possible results in small object detection.

\section{Architecture}

Choosing a multi-stage model for small object detection does not necessarily imply that the model will be demanding in terms of trainable parameters or challenging 
to train. Our approach strategically combines the two types of models in order to harness their individual strengths while maintaining manageable complexity 
and training efficiency. This integration ensures that our multi-stage detector not only achieves an elevated accuracy but also remains practical in terms of 
computational resources and training time, making it a viable solution for real-world applications where both performance and efficiency are critical.

In this section we are going to describe and explain the model we developed in detail with the aim of successfully tackling the Small Object Detection challenge. 
The model we created is named Extended Masked-Attention Mask Transformer, a name based on the models we decided to integrate being the Extended Feature Pyramid 
Network \cite{efpn} and the Masked-Attention Mask Transformer (MAMT) \cite{mamt2}.

A very interesting finding in the EFPN article is that over the past years, rapid development of deep learning has boosted the popularity of 
CNN-based detectors and the improved overall accuracy and efficiency. Unfortunately they still perform poorly when detecting small objects with a few pixels. 
Since CNN uses pooling layers repeatedly to extract advanced semantics, the pixels of small objects can be filtered out during the down-sampling process.

\newpage
Utilization of low-level features is one way to pick up information about small objects. The FPN is the first method to enhance features 
by fusing features from different levels and constructing feature pyramids, where upper feature maps are responsible for larger object detection, and lower 
feature maps are responsible for smaller object detection. Despite FPN's improvement multi-scale detection performance, the heuristic mapping mechanism 
between pyramid level and proposal size in FPN detectors may confuse small object detection, since small-sized objects must share the same feature map with 
medium-sized objects and some large-sized objects.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.55]{Figures/efpn-sod-mapping.jpg}
    \caption{Small Object Detection Mapping \cite{efpn}}
    \label{fig:efpn-sod-mapping}
\end{figure}

\newpage
\subsection{Extended Masked-Attention Mask Transformer}

Throughout our extensive research into advanced object detection models and architectures, it became an interesting idea that a hybrid approach would 
yield the most effective results, especially for the task of small object detection. The detailed analysis of various architectures led us to the strategic 
combination of two highly effective models: the Extended Feature Pyramid Network (EFPN) \ref{efpn-section} and the Masked-Attention Mask Transformer (MAMT) \ref{mamt}. 
This integration leverages the EFPN’s robust capability in generating precise multi-scale feature maps and initial bounding boxes and masks with great efficiency, 
with the MAMT’s advanced attention mechanisms that refine these masks for superior accuracy that is vital for the Small Object Detection task. 

In this section we are going to analyze our model, the Extended Masked-Attention Mask Transformer or EMAMT starting with the models architecture in the Figure
\ref{fig:emamt}.

\begin{figure}[h!]
    \includegraphics[scale=0.05]{Figures/EMAMT.png}
    \caption{Extended Masked-Attention Mask Transformer}
    \label{fig:emamt}
\end{figure}

Firstly we are going to analyze the EFPN part of the model in the lower left part of the Figure \ref{fig:emamt}, where the EFPN model has as a input,
an image of size $600 \times 600$. The reason for the selection of that size is the backbone we use to generate the rich feature maps for the model.
In our architecture we decided that a pre-trained feature extractor that combines great performance and light design was vital. This combination of characteristics
led us to the choice of EfficientNet \cite{efficient} model and more specifically the EfficientNet-B7 model that uses an input size of $600 \times 600$. Furthermore we decided 
to create five feature maps that have the sizes $(300, 300), (150, 150), (75, 75), (38, 38), (19, 19)$ in order to have a wide range of data for the model to use. 

Alongside the feature maps the EFPN also produces the bounding boxes and the initial binary masks that are going to be used by the MAMT detector. The 
bounding boxes and the masks are implemented on the most detailed feature map $P'2$ that is the result of the addition of the richest spatial map $C'2$ 
and the richest feature map created by the feature texture transfer $P'3$. At the same time we have included a anchor generator model that creates anchors, 
informed based on the mean and standard deviation of the size of the bounding boxes of each dataset in order to conclude the first cycle of detection, between the 
actual data, the anchors and the bounding boxes. In this way we are able to train the EFPN model in parallel to the MAMT model, achieving better convergence.
With the end of the EFPN model we are expecting a set of five  feature maps, a set of bounding boxes and the corresponding classed for each object 
and the initial binary masks. 

The next part of the EMAMT model is the patch and positional embedding we use in order to add to the model the  sense of spatial arrangement among the input 
image patches. Since transformers inherently lack any mechanism to recognize order or position, positional embeddings are crucial for providing this spatial 
context. The creation of the positional embeddings follows the exact same idea as the original paper of MAMT, since it was the implementation of the
sinusoidal positional embedding for images, adapted from the concept originally used in the transformer architecture for natural language processing.
This type of embedding generates position encodings based on sine and cosine functions of different frequencies.


At this point we have a set of five embeddings of the feature maps, a set of bounding boxes and the corresponding classes and the binary masks. Along all
that information the detector of this model we use a set number of queries that in this case is set to $100$, because we wanted to focus on reliable 
detecting a sensible number of objects. For images that contain more than $100$ objects and the annotations we decided to only use the first $100$ of them 
to train the model. Provided all of this data the Vision Transformer architecture can be seen in the Figure \ref{fig:emamt-trm}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.1]{Figures/emamt-encoder.png}
    \caption{Extended Masked-Attention Mask Transformer - Transformer Module}
    \label{fig:emamt-trm}
\end{figure}

\newpage
As mentioned in the section \ref{mamt} we have the hypothesis that local features are enough to update query features and context information can be 
gathered through self-attention. For this reason we use masked attention, a variant of cross-attention that only attends within the foreground region of 
the predicted mask for each query with the aim of better performance and faster convergence. More specifically the masked attention layer is used to focus 
the model’s attention selectively based on certain predefined or learned criteria like the object queries, typically aligning with relevant areas of the 
input image. For instance, in tasks like image segmentation, the masked attention layer might concentrate processing on areas within a specific region of 
interest (ROI), such as the foreground of an image versus the background. This selective attention helps the model to efficiently process only the most 
informative parts of the image, thereby optimizing computational cost.


Following the masked attention, the cross-attention layer has to integrate additional context or features from another source into the transformer’s workflow. 
When placed after a masked attention layer, the cross-attention layer can leverage the focused localized features processed by the masked attention and enrich 
these features with further contextual or supplementary data. 

Lastly after the attention mechanisms—both masked and cross-attention—the FFN acts as a point of non-linear transformation that further processes the refined, 
attention-focused features. This structure allows the FFN to introduce additional complexity and abstraction capabilities into the feature representations.


In order to maximize the capabilities of this model we had to create a custom loss function that takes into consideration all the data created by the model. 
The loss function for the Extended Mask2Former model is designed to jointly optimize object detection and instance segmentation tasks by integrating multiple 
loss components. The total loss \( L_{\text{total}} \) is computed as a weighted sum of the mask loss, bounding box regression loss, and classification loss:

\begin{equation}
    L_{\text{total}} = \lambda_{\text{mask}} \, L_{\text{mask}} + \lambda_{\text{bbox}} \, L_{\text{bbox}} + \lambda_{\text{class}} \, L_{\text{class}} \tag{9}    
\end{equation}

where:
\begin{itemize}
    \item \( \lambda_{\text{mask}} = 1 \) and \( \lambda_{\text{bbox}} = 1 \) are the weighting coefficients for the mask and bounding box losses, respectively, 
    emphasizing the importance of precise localization and segmentation of small objects.
    \item \( \lambda_{\text{class}} = 0.8 \), slightly less emphasized, is the weighting coefficient for the class loss, reflecting a somewhat lesser focus on class 
    accuracy relative to spatial accuracy.
\end{itemize}

\newpage
\begin{enumerate}
    \item \textbf{Mask Loss}: \\
    The mask loss measures the discrepancy between the predicted masks \( \hat{M} \) and the ground truth masks \( M \). It uses the Binary Cross-Entropy Loss with logits:
    \begin{equation}
        L_{\text{mask}} = \frac{1}{N} \sum_{i=1}^{N} \text{BCEWithLogits}\left( \hat{M}_i, \, M_i \right) \tag{10}
    \end{equation}

    - \( N \) is the number of matched instances. \\
    - The matching between predicted and ground truth masks is performed using the Hungarian algorithm based on a combined cost of mask Intersection over Union (IoU) 
    and classification cost.

    \item \textbf{Matching via Hungarian Algorithm}: \\

    To align predicted instances with ground truth instances, the Hungarian algorithm solves an assignment problem using a combined cost matrix \( \mathbf{C} \):
    
    \begin{equation}
        \mathbf{C} = \alpha \left( 1 - \text{IoU}\left( \hat{M}, \, M \right) \right) + \beta \left( -\log\left( \text{Softmax}\left( \hat{C}_{\text{mask}} \right) + \epsilon \right) \right) \tag{11}
    \end{equation}
    
    - \( \alpha \) and \( \beta \) are weighting coefficients for the IoU and classification cost components, respectively. \\ 
    - \( \text{IoU}\left( \hat{M}, \, M \right) \) computes the Intersection over Union between predicted and ground truth masks. \\
    - The negative log-likelihood term penalizes incorrect class predictions. \\
    - \( \epsilon \) is a small constant added to avoid taking the logarithm of zero. \\

    \item \textbf{Bounding Box Regression Loss}:

    This loss quantifies the error between the predicted bounding boxes \( \hat{B} \) and the regression targets \( B^* \) (encoded ground truth boxes relative to anchor boxes \( A \)) using the Smooth L1 Loss:
    
    \begin{equation}
        L_{\text{bbox}} = \frac{1}{K} \sum_{j=1}^{K} \text{SmoothL1}\left( \hat{B}_j, \, B^*_j \right) \tag{12}
    \end{equation}
    
    - \( K \) is the number of anchor boxes. \\ 
    - The regression targets \( B^* \) are obtained by encoding the matched ground truth boxes \( B \) with respect to the anchors \( A \):
    
      \begin{equation}
        B^* = \text{encode}\left( B, \, A \right) \tag{13}
      \end{equation}
    
    - Anchors are matched to ground truth boxes using an IoU-based matching strategy.

    \newpage
    \item \textbf{Classification Loss}:

    The classification loss evaluates the discrepancy between the combined class predictions \( \hat{C} \) and the ground truth class labels \( C \). 
    The predictions from both the bounding box branch \( \hat{C}_{\text{bbox}} \) and the mask branch \( \hat{C}_{\text{mask}} \) are combined by 
    taking the element-wise maximum of their softmax probabilities:
    
    \begin{equation}
        \hat{C} = \max\left( \text{Softmax}\left( \hat{C}_{\text{bbox}} \right), \, \text{Softmax}\left( \hat{C}_{\text{mask}} \right) \right) \tag{14}
    \end{equation}    
    The classification loss is then computed using the Cross-Entropy Loss:
    
    \begin{equation}
        L_{\text{class}} = \frac{1}{N} \sum_{i=1}^{N} \text{CrossEntropy}\left( \hat{C}_i, \, C_i \right) \tag{15}
    \end{equation}
    
    - \( N \) is the number of instances. \\
    - \( C_i \) is the ground truth class label for instance \( i \).

\end{enumerate}


This multi-task learning approach leverages the shared representations between tasks, leading to better generalization and performance compared to training each 
task separately. It allows the model to learn more robust and comprehensive features from the data, as the different tasks provide complementary information that 
enriches the learning process. By leveraging both mask predictions and bounding box regressions, the model maximizes the use of all available data from small objects. 
Specifically, this combined approach addresses the challenges posed by small objects, which often provide limited information due to their size. The masks capture 
fine-grained spatial details that might be overlooked in bounding box predictions, enhancing the accuracy of segmentation for small objects. Meanwhile, the bounding 
boxes provide coarse localization that can guide the mask predictions, improving robustness in detecting small objects. This synergy allows the model to effectively 
detect and segment small objects, enhancing both accuracy and robustness in scenarios where small object recognition is critical.

The table below, sorted by the number of parameters, provides a comparative look at various popular object detection models that we have presented in the 
section above. This table highlights models ranging from YOLOv3, known for its balance of speed and accuracy, to more complex architectures like Mask R-CNN. 
Notably, the Extended Masked-Attention Mask Transformer shows how recent advancements aim to reduce parameter counts while potentially enhancing model 
capabilities.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Model}                                 & \textbf{Number of Parameters (millions)} \\ \hline
    YOLO (You Only Look Once)                      & 62                                      \\ \hline
    Extended Masked-Attention Mask Transformer     & 76.6                                    \\ \hline
    Single Shot    Multibox Detector (SSD)         & Typically around 100                    \\ \hline
    Fast R-CNN                                     & Typically around 150                    \\ \hline
    Faster R-CNN                                   & Typically around 200                    \\ \hline
    FPN with Basic Detector                        & Typically around 200                    \\ \hline
    Mask R-CNN                                     & Typically around 250                    \\ \hline
    R-CNN                                          & Varies, typically less than 250         \\ \hline
    \end{tabular}
    \caption{Comparison of Object Detection Models Sorted by Number of Parameters}
    \label{tab:model_parameters}
\end{table}
    
