{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Extended Mask2Former UAV-SOD Drone Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os, json\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data_set_up import SOD_Data\n",
    "from models.extended_mask2former_model import ExtendedMask2Former\n",
    "from models.efpn_backbone.anchors import Anchors\n",
    "from src.helpers import train, validate, test\n",
    "\n",
    "\n",
    "# Import data paths\n",
    "map_path = \"src/code_map.json\"\n",
    "data_info_path = \"src/data_info/uav_data_preprocessing.json\"\n",
    "base_dir = \"data/uav_sod_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device we are going to load the model and the data\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up basic static data\n",
    "\n",
    "- Get the number of classes\n",
    "- Get the mean and standard deviation \n",
    "- Create the data paths for the [train, test, validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classes of the UAV-SOD Drone dataset\n",
    "map = open(map_path)\n",
    "data = json.load(map)\n",
    "classes = data[\"UAV_SOD_DRONE\"][\"CATEGORY_ID_TO_NAME\"]\n",
    "map.close() \n",
    "\n",
    "# The number of classes plus the background\n",
    "number_classes = len(classes) + 1\n",
    "\n",
    "\n",
    "# Load the mean and standard deviation for the train data\n",
    "map = open(data_info_path)\n",
    "data = json.load(map)\n",
    "mean = data[\"uav_data\"][\"mean\"]\n",
    "standard_deviation = data[\"uav_data\"][\"std\"]\n",
    "map.close() \n",
    "\n",
    "\n",
    "# Define train, test and validation path\n",
    "train_path = os.path.join(base_dir, \"train\")\n",
    "test_path = os.path.join(base_dir, \"test\")\n",
    "validation_path = os.path.join(base_dir, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Dataloader\n",
    "- Collate function\n",
    "- Data transformations\n",
    "- DataLoader and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transform function\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]),\n",
    "\n",
    "    \"test\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]), \n",
    "            \n",
    "    \"validation\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]) \n",
    "}\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset      = SOD_Data(train_path +\"/images\", train_path + \"/annotations\", data_transform[\"train\"])\n",
    "test_dataset       = SOD_Data(test_path + \"/images\", test_path  + \"/annotations\", data_transform[\"test\"])\n",
    "validation_dataset = SOD_Data(validation_path + \"/images\", validation_path + \"/annotations\", data_transform[\"validation\"])\n",
    "\n",
    "train_loader      = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader       = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_stats = train_dataset.analyze_bounding_boxes()\n",
    "\n",
    "\n",
    "mean_width = bbox_stats['mean_width']\n",
    "mean_height = bbox_stats['mean_height']\n",
    "\n",
    "std_width = bbox_stats['std_width']\n",
    "std_height = bbox_stats['std_height']\n",
    "\n",
    "\n",
    "\n",
    "# Print statistics\n",
    "print(\"Aspect Ratios:\", sorted(set(bbox_stats['aspect_ratios'])))\n",
    "print(\"Mean Width:\", bbox_stats['mean_width'])\n",
    "print(\"Mean Height:\", bbox_stats['mean_height'])\n",
    "print(\"Width Std Dev:\", bbox_stats['std_width'])\n",
    "print(\"Height Std Dev:\", bbox_stats['std_height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_shapes = [(150, 150), (75, 75), (38, 38), (19, 19), (10, 10)]\n",
    "\n",
    "scales = [mean_width - std_width, mean_width, mean_width + std_width, mean_height - std_height, mean_height, mean_height + std_height]\n",
    "scales = sorted(set([max(int(scale), 1) for scale in scales]))\n",
    "\n",
    "aspect_ratios = [0.75, 1.0, 1.25]\n",
    "\n",
    "anchors = torch.tensor(Anchors.generate_anchors(feature_map_shapes, scales, aspect_ratios), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtendedMask2Former(num_classes=number_classes).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss', 'precision', 'recall', 'AP', 'mAP'])\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_metrics, train_mAP = train(model, train_loader, optimizer, device, anchors, number_classes)\n",
    "    # val_loss, val_metrics, val_mAP = validate(model, validation_loader, device, anchors, number_classes)\n",
    "    \n",
    "    # Log metrics to DataFrame\n",
    "    # metrics_df = metrics_df.append({epoch + 1, train_loss, val_loss, train_metrics['precision'],  train_metrics['recall'], train_metrics['AP'],  train_mAP}, ignore_index=True) # type: ignore\n",
    "    \n",
    "    # print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, mAP: {train_mAP:.4f}')\n",
    "    \n",
    "    scheduler.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
