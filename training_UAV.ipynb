{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Extended Mask2Former UAV-SOD Drone Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, json\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data_set_up import SOD_Data\n",
    "from models.extended_mask2former_model import ExtendedMask2Former\n",
    "from models.efpn_backbone.anchors import Anchors\n",
    "\n",
    "\n",
    "# Import data paths\n",
    "map_path = \"src/code_map.json\"\n",
    "data_info_path = \"src/data_info/uav_data_preprocessing.json\"\n",
    "base_dir = \"data/uav_sod_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device we are going to load the model and the data\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up basic static data\n",
    "\n",
    "- Get the number of classes\n",
    "- Get the mean and standard deviation \n",
    "- Create the data paths for the [train, test, validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classes of the UAV-SOD Drone dataset\n",
    "map = open(map_path)\n",
    "data = json.load(map)\n",
    "classes = data[\"UAV_SOD_DRONE\"][\"CATEGORY_ID_TO_NAME\"]\n",
    "map.close() \n",
    "\n",
    "# The number of classes plus the background\n",
    "number_classes = len(classes) + 1\n",
    "\n",
    "\n",
    "# Load the mean and standard deviation for the train data\n",
    "map = open(data_info_path)\n",
    "data = json.load(map)\n",
    "mean = data[\"uav_data\"][\"mean\"]\n",
    "standard_deviation = data[\"uav_data\"][\"std\"]\n",
    "map.close() \n",
    "\n",
    "\n",
    "# Define train, test and validation path\n",
    "train_path = os.path.join(base_dir, \"train\")\n",
    "test_path = os.path.join(base_dir, \"test\")\n",
    "validation_path = os.path.join(base_dir, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Dataloader\n",
    "- Collate function\n",
    "- Data transformations\n",
    "- DataLoader and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transform function\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]),\n",
    "\n",
    "    \"test\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]), \n",
    "            \n",
    "    \"validation\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]) \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset      = SOD_Data(train_path +\"/images\", train_path + \"/annotations\", data_transform[\"train\"])\n",
    "test_dataset       = SOD_Data(test_path + \"/images\", test_path  + \"/annotations\", data_transform[\"test\"])\n",
    "validation_dataset = SOD_Data(validation_path + \"/images\", validation_path + \"/annotations\", data_transform[\"validation\"])\n",
    "\n",
    "train_loader      = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader       = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "            loss = model.compute_loss(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming feature_map_shapes, scales, and aspect_ratios are defined\n",
    "feature_map_shapes = [(38, 38)]  # Example shapes, adjust as needed\n",
    "scales = [32]\n",
    "aspect_ratios = [0.5, 1, 2]\n",
    "\n",
    "anchors = torch.tensor(Anchors.generate_anchors(feature_map_shapes, scales, aspect_ratios), dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    }
   ],
   "source": [
    "# Model, Optimizer and Training Loop setup\n",
    "model = ExtendedMask2Former(num_classes=number_classes).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: \n",
      "\n",
      "\n",
      "\n",
      "Predicted classes have type: <class 'torch.Tensor'>, shape:torch.Size([1, 100, 12]) and values: tensor([[[-0.1487,  0.6362, -0.4059,  ...,  0.2098, -0.1151,  0.3951],\n",
      "         [ 0.3056, -0.6809,  0.3058,  ...,  0.0929,  0.2725, -0.1422],\n",
      "         [ 0.3324,  0.1358, -0.4939,  ..., -0.1063,  0.2161, -0.5517],\n",
      "         ...,\n",
      "         [ 0.4634, -0.4760, -0.9415,  ..., -0.4130,  0.4977,  0.6163],\n",
      "         [ 0.0802,  0.2488, -0.3177,  ..., -0.2653, -0.4892,  0.0474],\n",
      "         [ 0.5932, -0.5929, -0.6051,  ..., -0.9471, -0.7681,  0.0504]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "\n",
      "Predicted masks have type: <class 'torch.Tensor'>, shape:torch.Size([1, 100, 300, 300]) and values: tensor([[[[-0.2432, -0.6747,  0.0358,  ..., -0.2660, -0.6948, -0.2972],\n",
      "          [-1.1692, -1.9498, -0.9090,  ..., -1.7550, -1.4841, -1.1843],\n",
      "          [-0.9276, -1.4161, -0.4205,  ..., -1.5546, -1.1311, -0.0659],\n",
      "          ...,\n",
      "          [-0.0962, -2.1769, -1.1772,  ..., -0.9011, -1.5522,  0.0263],\n",
      "          [-0.4669, -2.6392, -1.8552,  ..., -1.9774, -1.7092, -0.6319],\n",
      "          [-0.7025, -1.8342, -1.2522,  ..., -1.0257, -1.7457, -0.6225]],\n",
      "\n",
      "         [[-0.2707, -0.1013,  0.3006,  ..., -0.0330,  0.1258, -0.1761],\n",
      "          [-0.6226, -0.8959, -0.3129,  ..., -0.7409,  0.0102, -0.1994],\n",
      "          [-0.9833, -1.0718, -0.7187,  ..., -1.6075, -0.6413, -0.8751],\n",
      "          ...,\n",
      "          [-0.3358, -1.6796, -1.1878,  ..., -1.0041, -0.7559, -0.2147],\n",
      "          [-0.4116, -1.5298, -0.8361,  ..., -1.3161, -0.9371, -0.5814],\n",
      "          [-1.3070, -2.1139, -1.7274,  ..., -1.9302, -2.4361, -1.1889]],\n",
      "\n",
      "         [[-1.2294, -1.2876, -0.7099,  ..., -0.6624, -0.7072, -1.4204],\n",
      "          [-1.8513, -2.3945, -1.8429,  ..., -2.1043, -1.4362, -1.5013],\n",
      "          [-2.2962, -2.5268, -2.2273,  ..., -3.3873, -2.9768, -2.1383],\n",
      "          ...,\n",
      "          [-1.5656, -3.8453, -3.0672,  ..., -2.5931, -2.6065, -1.6316],\n",
      "          [-1.5793, -3.1619, -2.4795,  ..., -3.1123, -2.6824, -1.6575],\n",
      "          [-2.3061, -2.9504, -2.9935,  ..., -3.0378, -3.5762, -2.2825]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5591, -0.7190, -0.4130,  ..., -0.4114, -0.4472, -1.0504],\n",
      "          [-0.7842, -1.4823, -0.8864,  ..., -1.7048, -1.0164, -1.3396],\n",
      "          [-0.6374, -1.2283, -0.8424,  ..., -1.5069, -1.2489, -1.2765],\n",
      "          ...,\n",
      "          [ 0.0531, -1.5613, -1.2933,  ..., -1.2863, -1.3742, -0.7285],\n",
      "          [-0.5312, -2.3526, -2.0811,  ..., -2.0332, -1.5931, -1.0492],\n",
      "          [-0.7171, -1.5579, -1.4861,  ..., -1.5022, -1.5677, -1.0712]],\n",
      "\n",
      "         [[-0.7802, -1.0963, -0.5140,  ..., -0.8383, -1.0147, -0.8701],\n",
      "          [-1.2921, -2.2643, -1.4865,  ..., -2.5756, -1.9614, -1.0155],\n",
      "          [-1.5023, -2.2186, -1.5170,  ..., -3.5646, -2.5996, -1.4279],\n",
      "          ...,\n",
      "          [-0.7823, -3.3086, -2.5158,  ..., -1.9933, -2.4177, -0.8493],\n",
      "          [-1.1147, -3.1704, -2.2844,  ..., -2.6285, -2.5777, -0.8661],\n",
      "          [-1.2178, -2.8920, -2.2557,  ..., -2.1020, -2.7867, -1.3074]],\n",
      "\n",
      "         [[-0.7969, -1.1964, -0.6139,  ..., -1.0698, -1.2447, -1.7646],\n",
      "          [-1.2990, -1.9827, -1.2110,  ..., -2.1210, -1.5213, -1.8794],\n",
      "          [-1.4921, -2.0445, -1.4008,  ..., -2.7639, -2.0215, -1.9996],\n",
      "          ...,\n",
      "          [-1.0257, -3.0552, -2.3898,  ..., -2.0643, -2.1056, -1.2911],\n",
      "          [-1.2822, -2.9732, -2.3156,  ..., -2.7597, -2.5203, -1.8862],\n",
      "          [-1.6655, -2.7601, -2.1702,  ..., -2.1923, -2.6903, -1.6406]]]],\n",
      "       grad_fn=<ViewBackward0>) \n",
      "\n",
      "\n",
      "Predicted offsets have type: <class 'list'>, shape:5 and values: [tensor([[[[-0.1369, -1.1912, -1.2320,  ..., -1.1163, -1.0953,  0.3876],\n",
      "          [ 0.0033, -0.1701, -0.6569,  ..., -0.0954, -0.6404, -0.0980],\n",
      "          [ 0.6702, -0.4136, -1.0528,  ...,  0.3041, -0.4003,  0.2790],\n",
      "          ...,\n",
      "          [ 0.5798, -1.1607, -1.1302,  ..., -0.7105, -1.7849, -0.6051],\n",
      "          [ 0.7566,  0.4870, -0.9296,  ..., -0.1503, -1.1769, -0.0740],\n",
      "          [ 0.1550,  0.1148, -0.1482,  ..., -0.1021, -0.6630,  0.2893]],\n",
      "\n",
      "         [[-0.9789, -0.3195, -0.0596,  ...,  0.3463,  1.0742,  0.7758],\n",
      "          [-1.4491, -1.4836, -1.2863,  ...,  0.0151,  0.9224, -1.0153],\n",
      "          [-1.2549, -1.4625, -1.1746,  ..., -0.4815,  0.9804, -0.8417],\n",
      "          ...,\n",
      "          [-2.7966, -2.0637, -0.9307,  ..., -1.1231,  0.2501, -0.7612],\n",
      "          [ 0.0796, -0.0197,  0.5626,  ...,  0.3090,  1.1226, -0.3352],\n",
      "          [-1.7041, -2.2852, -1.9885,  ..., -2.3059, -1.4262, -2.7505]],\n",
      "\n",
      "         [[-1.0743, -1.3234, -1.4550,  ..., -2.5754, -2.4826, -2.2005],\n",
      "          [-0.1679, -0.6122, -0.3055,  ..., -1.0226, -0.8793, -0.1046],\n",
      "          [-0.1647, -0.2831, -0.5393,  ...,  0.4629,  0.4130,  0.7645],\n",
      "          ...,\n",
      "          [-1.3988, -1.8861, -1.8958,  ..., -0.8835, -1.5273, -0.1903],\n",
      "          [-0.3874, -1.5306, -1.8106,  ..., -1.5789, -2.0191, -1.7276],\n",
      "          [ 0.0368, -0.2793, -0.3295,  ..., -0.2664, -0.4428, -0.7962]],\n",
      "\n",
      "         [[ 0.6766,  1.0968,  0.9805,  ...,  0.2288,  0.7326,  0.1267],\n",
      "          [-0.1462, -1.0564, -1.0004,  ...,  0.0534, -0.5612, -0.2873],\n",
      "          [-0.1837, -0.6846, -1.1238,  ...,  0.5795, -0.0954,  0.0651],\n",
      "          ...,\n",
      "          [-0.9894, -0.7064, -0.7464,  ..., -1.3537, -0.4402, -1.0101],\n",
      "          [-0.3601, -0.3363, -0.2427,  ..., -0.8248, -0.6328, -1.2411],\n",
      "          [-0.8954, -0.9938, -1.3998,  ..., -1.1469, -1.1022, -1.4671]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ -4.3627,  -3.5387,  -3.8869,  ...,  -8.6332,  -8.2307,   1.4884],\n",
      "          [-12.8771, -18.9377, -19.9049,  ..., -11.8961, -11.9258,  -3.2590],\n",
      "          [-11.0841, -17.7097, -15.9061,  ..., -16.0683, -16.4940, -10.0903],\n",
      "          ...,\n",
      "          [-15.6889, -13.2961, -15.1730,  ..., -16.7986, -15.5765,  -5.2873],\n",
      "          [ -8.9901, -10.1374,  -7.9627,  ...,  -8.0091,  -9.5083,  -4.4390],\n",
      "          [ -5.8910, -11.4605, -11.2471,  ..., -12.3677, -13.3187,  -6.3341]],\n",
      "\n",
      "         [[ -4.1158,  -5.7640,  -7.6459,  ...,   0.5364,   1.1696,  -2.5418],\n",
      "          [  2.2454,  -1.6489,   1.4056,  ...,  -2.4168,  -2.2087,  -3.3820],\n",
      "          [  5.2127,   2.0054,   4.0033,  ...,  -5.2459,  -0.4465,  -3.8333],\n",
      "          ...,\n",
      "          [  2.3131,  -1.2168,  -6.3066,  ...,   3.4198,   4.7779,  -8.2392],\n",
      "          [ -1.0722, -12.2224, -13.1173,  ..., -10.4029,  -9.4885, -11.7909],\n",
      "          [ -2.5636,  -5.7528,  -3.5187,  ...,  -5.9782,  -3.7270, -12.3828]],\n",
      "\n",
      "         [[  1.7373,   1.8520,   0.2216,  ...,  -1.5780,  -0.9132,  -3.2153],\n",
      "          [ -3.3428,  -1.7016,  -2.6342,  ..., -13.0479, -15.4372,  -7.9672],\n",
      "          [  1.6552,   0.4760,  -4.1477,  ..., -11.0256, -12.5146,  -6.7177],\n",
      "          ...,\n",
      "          [ -3.4593,   2.1770,   4.3658,  ...,   0.3723,  -2.0150,   5.3741],\n",
      "          [ -1.7015,   2.5506,  -1.1587,  ...,  -5.8326,  -2.8553,   1.9872],\n",
      "          [ -0.1588,  -2.7524,  -3.2315,  ...,   3.9123,   4.1736,   9.2412]],\n",
      "\n",
      "         [[ -4.7146,  -0.7228,  -1.1191,  ...,  -4.0588,  -3.1144,  -4.3291],\n",
      "          [ -8.4568,  -5.5334,  -7.9016,  ...,   1.7129,   0.1257,  -4.7106],\n",
      "          [ -1.1195,  -3.4185,  -1.0832,  ...,  -2.1716,   0.4258,  -1.6450],\n",
      "          ...,\n",
      "          [ -3.8500,  -4.6273,   1.4074,  ...,   1.8480,   3.0999,  -7.4377],\n",
      "          [ -1.5418,  -5.8401,  -3.3403,  ...,   0.5174,  -3.1910,  -6.9833],\n",
      "          [ -0.7623,  -1.9476,  -2.7109,  ...,   3.0747,   3.1483,  -6.9553]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ -5.6152,  -4.6432,  -4.4486,  ...,  -6.1822,  -4.4564,   0.0842],\n",
      "          [ -5.2747,  -5.0428,  -6.7048,  ..., -14.4169, -13.2478,  -7.2984],\n",
      "          [ -4.6662,  -6.9914,  -8.2349,  ...,  -9.9815,  -2.3170,   3.8207],\n",
      "          ...,\n",
      "          [  0.9326,  -5.0154,  -3.5170,  ...,  -1.4362,  -3.3392,  -3.3737],\n",
      "          [ -2.7942,   1.0409,  -3.9032,  ...,   0.1083,  -4.8512,  -1.8415],\n",
      "          [ -5.5655,  -3.4113,  -5.3325,  ...,  -4.5882,  -5.9079,  -5.2350]],\n",
      "\n",
      "         [[ -3.9025,  -3.4784,  -2.5484,  ...,   0.2893,  -1.7560,  -3.5512],\n",
      "          [  1.4412,  -4.3751,  -5.8366,  ..., -11.9964, -13.8759,  -6.9015],\n",
      "          [ -1.9595,  -8.0567, -10.3867,  ..., -11.3821, -10.0923, -10.0007],\n",
      "          ...,\n",
      "          [ -6.5816,  -6.0036,  -7.9130,  ...,  -1.0254,   2.0014,   0.1218],\n",
      "          [ -5.1453, -11.3750, -11.2323,  ...,  -6.1977,  -5.5996,  -6.2705],\n",
      "          [ -5.3402, -11.1559,  -7.5899,  ...,  -8.4298,  -8.6855,  -6.8808]],\n",
      "\n",
      "         [[  0.0793,  -2.4962,  -1.5483,  ...,  -1.9250,  -2.6831,  -4.0660],\n",
      "          [ -2.2197,  -8.1448,  -5.4592,  ...,  -5.6050,  -6.9640,  -1.4775],\n",
      "          [  1.1117,  -8.0379,  -5.8947,  ...,  -6.8001,  -5.2175,   2.6924],\n",
      "          ...,\n",
      "          [  4.4094,   2.6033,  -2.9880,  ...,   3.6750,  -1.6484,   1.7420],\n",
      "          [ -1.6280,  -1.1287,  -3.7077,  ...,   4.4640,  -1.3599,   1.8553],\n",
      "          [  0.3765,  -4.2474,  -2.3083,  ...,  -2.2906,  -3.4125,   3.1980]],\n",
      "\n",
      "         [[ -7.1703,  -4.1733,  -5.9221,  ...,  -3.1810,   3.5084,  -1.9734],\n",
      "          [ -2.4709,  -0.1049,  -2.9482,  ...,   1.4895,   8.1007,   1.8244],\n",
      "          [ -2.2948,  -1.6423,   0.3624,  ...,   2.7364,   1.8289,  -1.9478],\n",
      "          ...,\n",
      "          [ -4.6670,  -2.6696,  -5.9748,  ...,  -5.2096,  -4.6614,  -6.4616],\n",
      "          [ -2.7943,   2.1632,   2.7453,  ...,   0.1211,   2.6471,  -2.9213],\n",
      "          [  3.3908,   1.1358,   2.4294,  ...,   4.4032,   5.1303,  -2.8741]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-4.0005, -2.4247, -1.5708,  ..., -4.4924, -1.7572, -0.1678],\n",
      "          [ 0.5329, -2.6599,  0.4240,  ..., -5.2432, -1.7052,  4.0868],\n",
      "          [-1.0349, -0.0503, -0.2658,  ..., -3.0837, -0.6792,  3.7745],\n",
      "          ...,\n",
      "          [-0.8947, -2.2872, -3.4638,  ...,  2.6502,  4.8551,  0.3114],\n",
      "          [ 1.5745, -0.1669, -1.6606,  ..., -2.6191,  0.6192, -0.5013],\n",
      "          [-1.3062, -0.0300, -0.2222,  ..., -1.7248, -1.9817, -2.4160]],\n",
      "\n",
      "         [[ 0.3578, -0.1546,  0.4922,  ..., -5.0502, -6.0982, -6.0377],\n",
      "          [ 1.2805, -2.3911,  0.5076,  ..., -3.4683, -3.4895, -6.9925],\n",
      "          [-0.8569, -4.3865, -1.4086,  ..., -0.3132, -2.6964, -3.1562],\n",
      "          ...,\n",
      "          [-3.5702, -8.7014, -1.0086,  ..., -1.6624, -1.6308,  1.1896],\n",
      "          [-2.3174, -6.9844, -3.5429,  ..., -0.1732, -1.7143, -0.1577],\n",
      "          [-3.2837, -1.9449, -3.5778,  ..., -2.3307, -2.4007, -3.1742]],\n",
      "\n",
      "         [[-1.5701, -0.1806,  1.2942,  ..., -0.6290,  0.6729,  0.1827],\n",
      "          [ 0.5022, -1.8740, -1.3023,  ..., -2.8033, -5.4269, -3.3210],\n",
      "          [ 0.3943,  0.3951,  0.7664,  ...,  0.3054, -3.4406, -0.0995],\n",
      "          ...,\n",
      "          [ 2.7358,  2.1155,  2.1786,  ...,  1.1935,  3.1609,  0.6322],\n",
      "          [ 0.3914,  3.2989,  5.3053,  ...,  3.0922,  2.4178,  1.1493],\n",
      "          [ 0.7235,  1.1128,  2.3004,  ...,  2.5948, -0.0797,  1.7929]],\n",
      "\n",
      "         [[-2.9183, -2.6877, -1.7159,  ...,  1.1118, -0.1568, -1.1877],\n",
      "          [-3.2233, -4.1425, -2.9182,  ...,  1.9043,  1.1476,  0.0893],\n",
      "          [-3.2998, -2.7988, -2.7619,  ...,  4.3350,  1.8356,  0.6340],\n",
      "          ...,\n",
      "          [ 0.3535, -4.6134, -3.7334,  ...,  1.5291,  1.5042, -0.9956],\n",
      "          [ 1.4589, -2.7928, -3.2003,  ...,  1.6485,  0.0909, -4.5703],\n",
      "          [-1.6774, -2.5129, -0.8835,  ...,  0.3025, -0.2350, -2.2023]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-6.6759e-01,  1.7330e+00,  9.8508e-01,  ..., -4.9155e-01,\n",
      "           -4.6577e-01,  1.1219e+00],\n",
      "          [-8.7877e-01,  1.2678e+00, -2.3658e-01,  ...,  2.9323e+00,\n",
      "            2.0129e+00,  9.3815e-01],\n",
      "          [ 1.8398e-01,  9.5334e-02,  2.4069e-01,  ...,  2.9197e+00,\n",
      "            2.1574e+00,  8.3752e-01],\n",
      "          ...,\n",
      "          [-2.1719e-01,  1.2880e+00,  2.8861e+00,  ...,  2.1189e-01,\n",
      "           -1.4257e+00, -8.7534e-02],\n",
      "          [ 6.3289e-01, -5.6818e-01,  6.9090e-01,  ..., -1.5181e+00,\n",
      "           -6.2325e-01,  6.7247e-01],\n",
      "          [-6.6679e-01,  1.0692e+00, -4.4919e-01,  ..., -1.9254e+00,\n",
      "           -8.6178e-02, -4.5162e-01]],\n",
      "\n",
      "         [[ 9.5571e-02, -8.6535e-02,  1.3184e+00,  ..., -6.1106e-01,\n",
      "           -1.4429e+00, -1.7737e+00],\n",
      "          [-7.7351e-01,  4.3747e-01,  9.7578e-01,  ..., -3.1407e-01,\n",
      "           -1.9385e+00, -2.4807e+00],\n",
      "          [-2.8479e-01, -3.8831e-01, -5.1797e-01,  ...,  4.5684e-01,\n",
      "           -1.0456e+00, -1.3473e+00],\n",
      "          ...,\n",
      "          [-1.0040e+00, -1.8184e+00, -1.6032e+00,  ..., -1.7820e+00,\n",
      "            1.2568e+00,  8.0592e-01],\n",
      "          [-2.4910e+00, -2.3400e+00, -1.3959e+00,  ..., -2.2071e+00,\n",
      "           -2.2319e-01, -9.7278e-01],\n",
      "          [-3.9458e-01, -2.8513e+00, -1.1970e+00,  ...,  1.9061e-01,\n",
      "           -2.7408e-01,  1.1904e+00]],\n",
      "\n",
      "         [[ 2.9396e-01,  3.1412e-01,  1.1293e+00,  ..., -9.8244e-02,\n",
      "           -2.1139e+00, -3.4634e+00],\n",
      "          [ 2.0931e-01, -4.8995e-02, -6.2668e-02,  ..., -3.0337e-02,\n",
      "           -1.1421e+00, -1.3636e+00],\n",
      "          [-1.2277e+00, -1.3837e+00, -5.4004e-01,  ...,  2.4258e+00,\n",
      "            4.4825e-01,  1.4082e-01],\n",
      "          ...,\n",
      "          [ 8.0189e-01,  1.7756e+00,  7.3009e-01,  ...,  2.9314e-01,\n",
      "            6.7657e-01, -7.7898e-01],\n",
      "          [ 5.0757e-01,  1.2854e+00,  4.5339e-01,  ...,  7.2151e-01,\n",
      "            2.2632e+00,  1.7131e+00],\n",
      "          [ 6.4491e-01, -4.1909e-02,  1.8365e-03,  ...,  1.1710e+00,\n",
      "            1.5570e+00,  1.7652e+00]],\n",
      "\n",
      "         [[-2.7753e-01, -2.4368e-01, -3.3687e-01,  ...,  6.4944e-01,\n",
      "            1.7406e+00,  1.6528e+00],\n",
      "          [-1.4443e+00, -2.5754e+00, -5.9625e-01,  ..., -1.1283e+00,\n",
      "            8.7823e-01,  2.8856e+00],\n",
      "          [-8.4699e-01, -6.2559e-01, -3.1444e-01,  ..., -2.2522e+00,\n",
      "           -2.0009e-01,  8.9653e-01],\n",
      "          ...,\n",
      "          [ 2.1008e+00, -8.1316e-01,  2.7610e-01,  ...,  8.2533e-01,\n",
      "            3.0469e-01,  1.5692e+00],\n",
      "          [ 1.2783e+00, -1.1605e+00,  3.6644e-01,  ...,  7.4340e-01,\n",
      "            1.7236e+00,  1.3323e+00],\n",
      "          [ 7.1656e-02,  3.1347e-01,  4.1043e-01,  ..., -1.2501e-01,\n",
      "            1.8996e+00, -2.3919e-01]]]], grad_fn=<ConvolutionBackward0>)] \n",
      "\n",
      "\n",
      "\n",
      "Target labels have type:<class 'torch.Tensor'>, size:torch.Size([21]) and values: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "\n",
      "\n",
      "Target masks have type:<class 'torch.Tensor'>, size:torch.Size([21, 300, 300]) and values: tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)\n",
      "\n",
      "\n",
      "\n",
      "Target boxes have type:<class 'torch.Tensor'>, size:torch.Size([21, 4]) and values: tensor([[ 50, 336,  69, 365],\n",
      "        [ 76, 332,  95, 357],\n",
      "        [ 93, 332, 108, 354],\n",
      "        [113, 301, 148, 328],\n",
      "        [ 75, 309, 105, 333],\n",
      "        [171, 436, 190, 456],\n",
      "        [221, 487, 239, 506],\n",
      "        [252, 473, 268, 488],\n",
      "        [280, 488, 302, 510],\n",
      "        [182, 513, 193, 530],\n",
      "        [371, 444, 382, 452],\n",
      "        [416, 460, 435, 482],\n",
      "        [388, 475, 406, 493],\n",
      "        [408, 488, 426, 509],\n",
      "        [404, 506, 418, 518],\n",
      "        [442, 483, 457, 489],\n",
      "        [459, 443, 472, 464],\n",
      "        [479, 503, 500, 526],\n",
      "        [484, 530, 507, 544],\n",
      "        [503, 515, 526, 542],\n",
      "        [525, 514, 547, 534]])\n",
      "\n",
      "\n",
      "\n",
      "Predicted resized labels have type:<class 'torch.Tensor'>, size:torch.Size([21, 12]) and values: tensor([[-0.1487,  0.6362, -0.4059, -0.2528,  0.6080, -0.1876,  0.8765,  0.2512,\n",
      "         -0.8940,  0.2098, -0.1151,  0.3951],\n",
      "        [ 0.3056, -0.6809,  0.3058, -0.1565,  0.3750, -0.6405, -0.0621,  0.1202,\n",
      "          0.4177,  0.0929,  0.2725, -0.1422],\n",
      "        [ 0.3324,  0.1358, -0.4939, -0.6644, -0.1316,  0.4543,  0.6552, -0.1129,\n",
      "          0.5140, -0.1063,  0.2161, -0.5517],\n",
      "        [ 0.1984,  0.0478, -0.4362, -0.4080,  0.3535,  0.3712,  0.4306,  0.4470,\n",
      "         -0.0329,  0.2525, -0.0312,  0.3209],\n",
      "        [ 0.3113, -0.6619, -0.3404, -0.7053,  0.7612, -0.7429,  0.0456,  0.0860,\n",
      "          0.2064,  0.0062, -0.7084,  0.0656],\n",
      "        [-0.5409,  0.4705, -0.8966, -0.4480,  1.0419, -0.1083,  0.0711,  0.2887,\n",
      "         -0.5151, -0.1087, -0.0092, -0.2388],\n",
      "        [-0.0445,  0.6048, -0.7565, -0.9402,  0.5764,  0.1670,  0.0929,  0.8960,\n",
      "         -0.0446, -0.0450, -0.1150, -0.0603],\n",
      "        [ 0.5284,  0.1459, -0.4958, -0.3722, -0.0035,  0.2636,  0.0348, -0.0530,\n",
      "          1.2001,  0.1382,  0.1957, -0.2337],\n",
      "        [ 0.3644,  0.4086,  0.2348, -0.9134,  0.5724,  0.3260,  0.1806,  0.4570,\n",
      "         -0.2043,  0.5434,  0.5970, -0.4860],\n",
      "        [ 0.2334,  0.0965, -1.1444, -0.5748,  0.2664, -0.1579,  0.5615,  0.1279,\n",
      "          0.2533,  0.4124, -0.1860, -0.5358],\n",
      "        [-0.0669, -0.3874, -1.3699, -0.5362,  0.2009,  0.1269,  0.5355,  0.1000,\n",
      "         -0.0259,  0.5940, -0.8796, -0.2870],\n",
      "        [ 0.0664,  0.4563, -0.6465, -0.8424, -0.2997, -0.3699, -0.2807, -0.8590,\n",
      "          0.4980,  0.2260, -0.0579, -0.2060],\n",
      "        [ 0.3201, -0.1606, -0.1965, -0.8499, -0.1834, -0.9297,  0.2576,  0.0325,\n",
      "         -0.0066,  0.0160, -0.5823,  0.1636],\n",
      "        [ 0.2152,  0.0674, -0.5178, -1.0786,  0.4495, -0.3166,  0.3570,  0.3955,\n",
      "          0.9468,  0.0150, -0.7005, -0.4347],\n",
      "        [-0.1336,  0.6686, -0.3562, -0.3631,  0.3411,  0.0316,  0.1819,  0.5402,\n",
      "         -0.0851, -0.0873, -0.0665,  0.5980],\n",
      "        [ 0.5432, -0.2447, -0.1763, -0.5595,  0.4821, -0.3094,  0.7853,  0.2281,\n",
      "         -0.1080, -0.1422, -0.5456, -0.1439],\n",
      "        [ 0.3028, -0.1739, -0.3005, -0.4809,  0.3778, -1.0072, -0.1304,  0.3192,\n",
      "         -0.7889,  0.2607,  0.7697, -0.0274],\n",
      "        [ 0.2224,  0.3648, -0.7752, -0.3753,  0.1841, -0.7298,  0.2066, -0.0933,\n",
      "          0.0442,  0.0697,  0.1217, -0.3036],\n",
      "        [ 0.3571,  0.0128, -0.6743, -0.6755,  0.7865, -0.3267,  0.7598,  1.1899,\n",
      "         -0.4187, -0.3079, -0.3823,  0.2032],\n",
      "        [-0.4873,  0.3624, -0.8613, -0.2624,  0.2817, -0.2196,  0.7018,  0.5347,\n",
      "         -0.3374,  0.3243,  0.0295,  0.1752],\n",
      "        [ 0.1898,  0.0492,  0.1969, -0.1215,  0.8353, -0.4786,  0.1268,  0.0370,\n",
      "         -0.1691, -0.5452,  0.0644, -0.3628]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# def validate(model, val_loader, device, anchors):\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     model.eval()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     val_loss = 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#             test_loss += loss.item()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     return test_loss / len(test_loader)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 42\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# validation_loss = validate(model, validation_loader, device, anchors)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# print(f'Validation Loss: {validation_loss:.4f}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, device, anchors)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/MCs_Thesis/SOD_Thesis/models/extended_mask2former_model.py:80\u001b[0m, in \u001b[0;36mExtendedMask2Former.compute_loss\u001b[0;34m(self, predictions, targets, anchors, class_weight, bounding_box_weight, mask_weight)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted resized labels have type:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, size:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and values: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(pred_logits_resized), pred_logits_resized\u001b[38;5;241m.\u001b[39msize(), pred_logits_resized))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Decode the predicted bounding box offsets using the anchors\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m pred_boxes_resized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_boxes(\u001b[43mpredicted_offsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_objects\u001b[49m\u001b[43m]\u001b[49m, anchors[:num_objects])\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Compute classification loss, bounding box loss, and mask loss for each target\u001b[39;00m\n\u001b[1;32m     83\u001b[0m total_class_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_loss(pred_logits_resized, target_labels) \u001b[38;5;241m*\u001b[39m class_weight\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, optimizer, device, anchors):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = model.compute_loss(outputs, targets, anchors)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "# def validate(model, val_loader, device, anchors):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in val_loader:\n",
    "#             images = torch.stack(images).to(device)\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#             outputs, _ = model(images)\n",
    "#             loss = model.compute_loss(outputs, targets, anchors)\n",
    "#             val_loss += loss.item()\n",
    "#     return val_loss / len(val_loader)\n",
    "\n",
    "# def test(model, test_loader, device, anchors):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in test_loader:\n",
    "#             images = torch.stack(images).to(device)\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#             outputs, _ = model(images)\n",
    "#             loss = model.compute_loss(outputs, targets, anchors)\n",
    "#             test_loss += loss.item()\n",
    "#     return test_loss / len(test_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, device, anchors)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # validation_loss = validate(model, validation_loader, device, anchors)\n",
    "    # print(f'Validation Loss: {validation_loss:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# # Test the model\n",
    "# test_loss = test(model, test_loader, device, anchors)\n",
    "# print(f'Test Loss: {test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
