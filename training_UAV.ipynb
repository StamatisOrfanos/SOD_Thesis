{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Extended Mask2Former UAV-SOD Drone Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, json\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data_set_up import SOD_Data\n",
    "from models.extended_mask2former_model import ExtendedMask2Former\n",
    "from models.efpn_backbone.anchors import Anchors\n",
    "\n",
    "\n",
    "# Import data paths\n",
    "map_path = \"src/code_map.json\"\n",
    "data_info_path = \"src/data_info/uav_data_preprocessing.json\"\n",
    "base_dir = \"data/uav_sod_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device we are going to load the model and the data\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up basic static data\n",
    "\n",
    "- Get the number of classes\n",
    "- Get the mean and standard deviation \n",
    "- Create the data paths for the [train, test, validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classes of the UAV-SOD Drone dataset\n",
    "map = open(map_path)\n",
    "data = json.load(map)\n",
    "classes = data[\"UAV_SOD_DRONE\"][\"CATEGORY_ID_TO_NAME\"]\n",
    "map.close() \n",
    "\n",
    "# The number of classes plus the background\n",
    "number_classes = len(classes) + 1\n",
    "\n",
    "\n",
    "# Load the mean and standard deviation for the train data\n",
    "map = open(data_info_path)\n",
    "data = json.load(map)\n",
    "mean = data[\"uav_data\"][\"mean\"]\n",
    "standard_deviation = data[\"uav_data\"][\"std\"]\n",
    "map.close() \n",
    "\n",
    "\n",
    "# Define train, test and validation path\n",
    "train_path = os.path.join(base_dir, \"train\")\n",
    "test_path = os.path.join(base_dir, \"test\")\n",
    "validation_path = os.path.join(base_dir, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Dataloader\n",
    "- Collate function\n",
    "- Data transformations\n",
    "- DataLoader and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transform function\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]),\n",
    "\n",
    "    \"test\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]), \n",
    "            \n",
    "    \"validation\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]) \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset      = SOD_Data(train_path +\"/images\", train_path + \"/annotations\", data_transform[\"train\"])\n",
    "test_dataset       = SOD_Data(test_path + \"/images\", test_path  + \"/annotations\", data_transform[\"test\"])\n",
    "validation_dataset = SOD_Data(validation_path + \"/images\", validation_path + \"/annotations\", data_transform[\"validation\"])\n",
    "\n",
    "train_loader      = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader       = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "            loss = model.compute_loss(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming feature_map_shapes, scales, and aspect_ratios are defined\n",
    "feature_map_shapes = [(38, 38)]  # Example shapes, adjust as needed\n",
    "scales = [32]\n",
    "aspect_ratios = [0.5, 1, 2]\n",
    "\n",
    "anchors = torch.tensor(Anchors.generate_anchors(feature_map_shapes, scales, aspect_ratios), dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    }
   ],
   "source": [
    "# Model, Optimizer and Training Loop setup\n",
    "model = ExtendedMask2Former(num_classes=number_classes).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted offsets have type: <class 'torch.Tensor'>, shape:torch.Size([1, 4, 300, 300]) and values: tensor([[[[-0.5831, -0.1796, -0.0210,  ..., -0.2406, -1.0099, -0.4650],\n",
      "          [-0.5722,  0.2312,  0.2199,  ...,  0.2599,  0.2155,  0.3269],\n",
      "          [-0.2227,  0.5560,  0.5644,  ...,  1.0210,  0.6433,  0.8166],\n",
      "          ...,\n",
      "          [-0.5424,  0.2209,  0.3058,  ...,  0.5295,  0.3271,  0.9858],\n",
      "          [-0.2271,  0.6366,  1.0770,  ...,  1.1703,  1.0773,  0.9473],\n",
      "          [-1.3787, -1.0291, -1.0021,  ..., -0.4152, -0.2994,  0.3248]],\n",
      "\n",
      "         [[ 0.2041,  0.4356,  0.7073,  ...,  0.9764,  0.9260,  1.6755],\n",
      "          [-1.0625, -1.4357, -1.0329,  ..., -0.7635, -0.9092,  0.4284],\n",
      "          [-1.2990, -1.2378, -1.0368,  ..., -1.0173, -0.7501,  0.6204],\n",
      "          ...,\n",
      "          [-1.0794, -0.9195, -0.8900,  ..., -1.0832, -1.0589,  0.2916],\n",
      "          [-1.3883, -1.6016, -1.1372,  ..., -1.1190, -0.8145,  0.0438],\n",
      "          [-1.9999, -1.6903, -1.5308,  ..., -1.4728, -0.5162,  0.6776]],\n",
      "\n",
      "         [[-0.9858, -0.1240, -0.4325,  ..., -0.5222,  0.0193, -1.2785],\n",
      "          [-1.4410, -0.1532, -0.2840,  ..., -0.3228,  0.6095, -2.1579],\n",
      "          [-1.4403, -0.3557, -0.4519,  ..., -0.4195,  0.4478, -2.0880],\n",
      "          ...,\n",
      "          [-1.1135, -0.3454, -0.0716,  ...,  0.0824,  0.8357, -1.4359],\n",
      "          [-1.5269, -1.2635, -1.2834,  ..., -1.0710, -0.5749, -2.5428],\n",
      "          [-1.8343, -0.5860, -0.4186,  ..., -0.2869, -0.2540, -0.1124]],\n",
      "\n",
      "         [[-2.4783, -2.8851, -2.6898,  ..., -2.7638, -2.6959, -1.1708],\n",
      "          [-1.1271, -2.4180, -2.2629,  ..., -2.2378, -2.0164, -0.9173],\n",
      "          [-1.8213, -3.1854, -3.1340,  ..., -2.6710, -2.8693, -1.5495],\n",
      "          ...,\n",
      "          [-1.6695, -2.7108, -2.5579,  ..., -2.8445, -2.8774, -1.5759],\n",
      "          [-1.2418, -2.7410, -2.2208,  ..., -2.2983, -2.4444, -1.5611],\n",
      "          [-1.1419, -3.0043, -2.5755,  ..., -2.5677, -2.8184, -1.4963]]]],\n",
      "       grad_fn=<ConvolutionBackward0>) \n",
      "\n",
      "\n",
      "\n",
      "Target boxes have type:<class 'torch.Tensor'>, size:torch.Size([11, 4]) and values: tensor([[ 14, 268,  31, 294],\n",
      "        [ 29, 256,  80, 304],\n",
      "        [ 79, 255, 120, 300],\n",
      "        [161, 268, 215, 321],\n",
      "        [ 18, 261,  32, 270],\n",
      "        [196, 296, 211, 310],\n",
      "        [ 79, 390, 129, 435],\n",
      "        [131, 365, 173, 413],\n",
      "        [485, 349, 544, 407],\n",
      "        [497, 331, 523, 356],\n",
      "        [ 73, 223,  87, 232]])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " First Parameter: [tensor([[[[-0.5831, -0.1796, -0.0210,  ..., -0.2406, -1.0099, -0.4650],\n",
      "          [-0.5722,  0.2312,  0.2199,  ...,  0.2599,  0.2155,  0.3269],\n",
      "          [-0.2227,  0.5560,  0.5644,  ...,  1.0210,  0.6433,  0.8166],\n",
      "          ...,\n",
      "          [-0.5424,  0.2209,  0.3058,  ...,  0.5295,  0.3271,  0.9858],\n",
      "          [-0.2271,  0.6366,  1.0770,  ...,  1.1703,  1.0773,  0.9473],\n",
      "          [-1.3787, -1.0291, -1.0021,  ..., -0.4152, -0.2994,  0.3248]],\n",
      "\n",
      "         [[ 0.2041,  0.4356,  0.7073,  ...,  0.9764,  0.9260,  1.6755],\n",
      "          [-1.0625, -1.4357, -1.0329,  ..., -0.7635, -0.9092,  0.4284],\n",
      "          [-1.2990, -1.2378, -1.0368,  ..., -1.0173, -0.7501,  0.6204],\n",
      "          ...,\n",
      "          [-1.0794, -0.9195, -0.8900,  ..., -1.0832, -1.0589,  0.2916],\n",
      "          [-1.3883, -1.6016, -1.1372,  ..., -1.1190, -0.8145,  0.0438],\n",
      "          [-1.9999, -1.6903, -1.5308,  ..., -1.4728, -0.5162,  0.6776]],\n",
      "\n",
      "         [[-0.9858, -0.1240, -0.4325,  ..., -0.5222,  0.0193, -1.2785],\n",
      "          [-1.4410, -0.1532, -0.2840,  ..., -0.3228,  0.6095, -2.1579],\n",
      "          [-1.4403, -0.3557, -0.4519,  ..., -0.4195,  0.4478, -2.0880],\n",
      "          ...,\n",
      "          [-1.1135, -0.3454, -0.0716,  ...,  0.0824,  0.8357, -1.4359],\n",
      "          [-1.5269, -1.2635, -1.2834,  ..., -1.0710, -0.5749, -2.5428],\n",
      "          [-1.8343, -0.5860, -0.4186,  ..., -0.2869, -0.2540, -0.1124]],\n",
      "\n",
      "         [[-2.4783, -2.8851, -2.6898,  ..., -2.7638, -2.6959, -1.1708],\n",
      "          [-1.1271, -2.4180, -2.2629,  ..., -2.2378, -2.0164, -0.9173],\n",
      "          [-1.8213, -3.1854, -3.1340,  ..., -2.6710, -2.8693, -1.5495],\n",
      "          ...,\n",
      "          [-1.6695, -2.7108, -2.5579,  ..., -2.8445, -2.8774, -1.5759],\n",
      "          [-1.2418, -2.7410, -2.2208,  ..., -2.2983, -2.4444, -1.5611],\n",
      "          [-1.1419, -3.0043, -2.5755,  ..., -2.5677, -2.8184, -1.4963]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[-3.1935e+00, -6.9206e+00, -4.5820e+00,  ..., -3.0120e+00,\n",
      "           -2.0177e+00, -1.5567e+00],\n",
      "          [ 2.9784e-01, -6.8786e+00, -4.8087e+00,  ..., -2.5284e+00,\n",
      "           -3.4374e-01, -9.2559e-01],\n",
      "          [ 9.6813e-01, -5.2914e+00, -4.2065e+00,  ..., -1.7879e+00,\n",
      "            9.0155e-01, -1.9186e+00],\n",
      "          ...,\n",
      "          [ 6.6233e-01, -4.0139e+00, -1.8483e+00,  ..., -6.2919e+00,\n",
      "           -4.6659e+00, -6.1638e+00],\n",
      "          [-2.7482e-01, -2.9493e+00, -2.4698e+00,  ..., -6.3521e+00,\n",
      "           -4.3915e+00, -7.5551e-01],\n",
      "          [-3.6365e-01, -2.6950e-01,  8.8521e-01,  ...,  2.0710e-02,\n",
      "            2.3945e+00,  4.1427e+00]],\n",
      "\n",
      "         [[-5.9437e+00, -6.9504e+00, -5.6692e+00,  ..., -5.2017e+00,\n",
      "           -5.2824e+00,  2.1067e-01],\n",
      "          [-5.3558e-01,  7.9710e-04, -6.6009e-01,  ...,  1.9614e+00,\n",
      "            5.8974e-01,  2.8497e+00],\n",
      "          [-3.4341e+00, -2.6586e+00, -3.3687e+00,  ..., -4.4847e-01,\n",
      "           -7.3415e-01,  1.7185e+00],\n",
      "          ...,\n",
      "          [-9.7771e-01,  2.2060e+00,  7.1296e-01,  ..., -7.7701e-01,\n",
      "            5.4059e-01,  5.0226e+00],\n",
      "          [-2.9352e+00,  1.4098e+00, -1.3961e-01,  ...,  2.6825e-01,\n",
      "            2.5468e+00,  5.3584e+00],\n",
      "          [ 4.1123e+00,  2.2539e+00,  3.0733e+00,  ...,  2.0326e+00,\n",
      "            2.6263e+00,  2.3223e+00]],\n",
      "\n",
      "         [[-1.0253e+01, -1.1655e+01, -1.2061e+01,  ..., -1.5495e+01,\n",
      "           -1.8946e+01, -1.0789e+01],\n",
      "          [-6.3863e+00, -9.2901e+00, -9.6307e+00,  ..., -9.7837e+00,\n",
      "           -1.2945e+01, -6.7449e+00],\n",
      "          [-7.7309e+00, -9.1260e+00, -1.1405e+01,  ..., -9.1452e+00,\n",
      "           -1.1575e+01, -6.8052e+00],\n",
      "          ...,\n",
      "          [-1.0497e+01, -1.0632e+01, -1.2071e+01,  ..., -1.0857e+01,\n",
      "           -1.3286e+01, -8.8341e+00],\n",
      "          [-1.3095e+01, -1.4742e+01, -1.4816e+01,  ..., -1.1123e+01,\n",
      "           -1.5678e+01, -1.0105e+01],\n",
      "          [-1.1164e+00, -1.7288e+00, -6.0099e-01,  ...,  1.9199e+00,\n",
      "            2.9308e-01, -1.2519e+00]],\n",
      "\n",
      "         [[ 1.4408e+00, -2.4786e-01,  3.3111e+00,  ...,  2.6045e+00,\n",
      "            1.8122e+00,  5.9398e-01],\n",
      "          [-2.2999e+00, -4.5307e+00, -1.3979e+00,  ..., -3.1267e+00,\n",
      "           -2.6472e+00,  8.0297e-01],\n",
      "          [-5.2267e-01, -1.2524e+00,  1.1261e+00,  ..., -6.7242e-01,\n",
      "           -2.3203e+00,  1.5142e+00],\n",
      "          ...,\n",
      "          [-3.0910e-01, -2.7137e+00, -1.8716e+00,  ..., -4.8943e+00,\n",
      "           -5.2787e+00, -4.1354e+00],\n",
      "          [-2.3358e+00, -3.2478e+00, -3.1952e+00,  ..., -7.8158e+00,\n",
      "           -8.5398e+00, -3.4160e+00],\n",
      "          [-8.4593e-01, -2.8008e+00, -5.3844e-01,  ..., -3.7371e+00,\n",
      "           -3.3215e+00, -2.7559e+00]]]], grad_fn=<ConvolutionBackward0>), tensor([[[[-3.5404e-01, -1.7302e+00, -1.5198e+00,  ...,  5.9472e-01,\n",
      "            3.1767e-01,  1.0193e+00],\n",
      "          [ 6.9631e-01, -1.0105e+00,  8.6305e-02,  ...,  1.9314e+00,\n",
      "            2.0098e+00,  2.7228e+00],\n",
      "          [ 3.9853e-01,  7.4002e-01,  7.3707e-01,  ...,  1.0637e+00,\n",
      "            1.6318e+00,  1.9053e+00],\n",
      "          ...,\n",
      "          [ 1.1536e+00,  6.1674e-01,  7.8221e-01,  ...,  1.7850e+00,\n",
      "            2.1535e-01, -1.0735e+00],\n",
      "          [-5.4330e-02, -5.2862e-01, -3.3167e-01,  ..., -1.9376e+00,\n",
      "           -3.0427e+00, -3.6424e+00],\n",
      "          [-2.1170e+00,  8.1992e-03, -1.7737e-01,  ..., -1.1109e+00,\n",
      "           -1.6968e+00,  2.2661e-01]],\n",
      "\n",
      "         [[-4.3693e+00, -4.9272e+00, -3.2026e+00,  ..., -5.4732e+00,\n",
      "           -4.0691e+00, -2.0867e+00],\n",
      "          [-5.1291e+00, -4.6430e+00, -2.0489e+00,  ..., -4.2244e+00,\n",
      "           -3.4712e+00, -1.1979e+00],\n",
      "          [-6.1536e+00, -3.8013e+00, -4.1781e-01,  ..., -4.0522e+00,\n",
      "           -4.1792e+00, -1.2704e-01],\n",
      "          ...,\n",
      "          [-4.7832e+00, -3.1269e+00, -1.2321e+00,  ..., -6.4297e+00,\n",
      "           -6.8297e+00, -3.7606e-01],\n",
      "          [-2.4013e+00, -2.5888e-01,  3.5329e-01,  ..., -4.9145e+00,\n",
      "           -4.6278e+00, -2.4039e-01],\n",
      "          [ 2.0665e+00,  3.1829e-01,  3.6247e-02,  ..., -1.0110e+00,\n",
      "           -3.3997e-01,  3.0439e-01]],\n",
      "\n",
      "         [[-7.9599e+00, -7.7102e+00, -7.1322e+00,  ..., -5.2334e+00,\n",
      "           -8.2253e+00, -5.2233e+00],\n",
      "          [-7.2382e+00, -6.0527e+00, -5.0135e+00,  ..., -3.1113e+00,\n",
      "           -4.4132e+00, -6.7779e-01],\n",
      "          [-7.5425e+00, -4.8569e+00, -4.7201e+00,  ..., -5.7722e+00,\n",
      "           -6.7665e+00, -1.9337e+00],\n",
      "          ...,\n",
      "          [-5.5375e+00, -3.6380e+00, -2.6117e+00,  ..., -3.3128e+00,\n",
      "           -3.4706e+00, -2.1085e+00],\n",
      "          [-8.1958e+00, -7.5271e+00, -7.5150e+00,  ..., -7.1038e+00,\n",
      "           -8.4434e+00, -2.9860e+00],\n",
      "          [-4.3592e+00, -2.0669e+00, -1.9881e+00,  ..., -2.5518e+00,\n",
      "           -1.4779e-01,  1.1003e-01]],\n",
      "\n",
      "         [[ 1.9643e+00,  2.0134e+00,  2.4600e+00,  ...,  2.8483e+00,\n",
      "            2.9017e+00, -1.1070e-01],\n",
      "          [ 1.4596e+00,  2.0154e+00,  1.6518e+00,  ...,  2.1247e+00,\n",
      "            3.6502e+00,  3.7422e+00],\n",
      "          [ 4.1401e+00,  3.6068e+00,  2.8743e+00,  ..., -3.9938e-01,\n",
      "            2.8178e-01,  1.9186e+00],\n",
      "          ...,\n",
      "          [ 2.3072e+00,  2.8141e+00,  2.4533e+00,  ...,  2.1919e+00,\n",
      "            1.3614e+00,  1.1804e+00],\n",
      "          [ 2.9067e+00,  6.1102e-01,  1.9271e-01,  ..., -1.5543e-02,\n",
      "            1.3075e-01, -1.5342e+00],\n",
      "          [ 1.5799e+00,  1.7053e+00,  9.9262e-01,  ..., -1.8022e+00,\n",
      "           -1.7389e+00,  8.7232e-01]]]], grad_fn=<ConvolutionBackward0>), tensor([[[[-1.0097, -1.4026, -0.6482,  ...,  0.0141, -1.3688, -0.2365],\n",
      "          [-0.3585, -0.0522, -1.6533,  ..., -2.3088, -2.8460, -1.8059],\n",
      "          [-0.7055, -2.0542, -3.1354,  ..., -1.2398, -0.5404,  0.3375],\n",
      "          ...,\n",
      "          [ 3.0014, -0.7707,  0.6330,  ..., -0.1250, -1.2472, -0.8426],\n",
      "          [-3.2366, -2.9637, -2.2641,  ..., -3.1195, -5.4942, -5.3778],\n",
      "          [-2.4596, -1.3664, -2.3888,  ..., -2.2497, -3.4118, -0.7139]],\n",
      "\n",
      "         [[-3.2756, -1.5103, -3.6359,  ..., -2.5924, -2.8168, -2.0279],\n",
      "          [-1.0393,  3.4830,  0.3804,  ..., -0.8712, -0.6774, -1.5968],\n",
      "          [-3.5068,  1.5402, -0.3418,  ..., -3.7344, -1.5013, -1.2231],\n",
      "          ...,\n",
      "          [-4.1185,  0.9099,  0.1341,  ..., -1.6027, -4.5925, -0.2734],\n",
      "          [-3.9852, -0.6623, -1.0798,  ..., -4.0872, -5.0012, -3.0954],\n",
      "          [ 1.9995,  1.2180, -0.4522,  ..., -0.4846,  0.5214,  0.3220]],\n",
      "\n",
      "         [[-6.8708, -5.3685, -3.7083,  ..., -4.0340, -5.6826, -4.7223],\n",
      "          [-4.1960, -1.9901, -2.8126,  ..., -3.8299, -4.5671, -0.3302],\n",
      "          [-5.4546, -1.3125, -0.8357,  ..., -4.3690, -5.9315, -0.8465],\n",
      "          ...,\n",
      "          [-6.6380, -4.5844, -1.6294,  ..., -3.3011,  0.5694, -2.1435],\n",
      "          [-5.9011, -2.4518, -2.6339,  ..., -4.1491, -3.7311, -2.5328],\n",
      "          [-3.5431,  0.1213, -1.0501,  ..., -2.5810, -2.6224, -1.0530]],\n",
      "\n",
      "         [[-0.9134,  2.0916,  2.1269,  ...,  1.4520,  2.1013,  1.6471],\n",
      "          [-1.9946, -0.7240, -1.3477,  ..., -1.8829, -0.1931,  2.3343],\n",
      "          [-2.6503, -2.7748, -2.7770,  ..., -2.9151, -1.7986,  0.1984],\n",
      "          ...,\n",
      "          [-1.1159,  2.4059,  1.8623,  ...,  0.1118, -0.2465, -2.0301],\n",
      "          [ 0.5589,  2.5659,  0.8901,  ..., -1.3825,  0.3065, -0.2043],\n",
      "          [-1.1955,  1.3604, -0.4367,  ..., -3.1359, -2.7429,  0.9781]]]],\n",
      "       grad_fn=<ConvolutionBackward0>), tensor([[[[ 1.5576e-01, -7.0376e-02,  7.2248e-01,  ...,  5.3064e-01,\n",
      "            1.8719e-01,  1.3313e-01],\n",
      "          [ 4.6622e-01, -3.6478e-01, -6.4521e-01,  ...,  1.0625e+00,\n",
      "            1.2654e-01, -6.4375e-02],\n",
      "          [ 4.0132e-01, -7.4179e-01, -8.1407e-01,  ...,  1.4593e+00,\n",
      "            1.0971e+00,  1.2430e+00],\n",
      "          ...,\n",
      "          [ 7.6588e-01,  1.3622e+00,  1.6207e+00,  ...,  3.1465e-01,\n",
      "            1.5740e-01, -1.6333e+00],\n",
      "          [ 1.1498e+00,  3.2040e-01,  8.1358e-01,  ...,  6.5722e-01,\n",
      "           -1.1690e+00, -1.1969e+00],\n",
      "          [-4.3510e-01,  6.9949e-01,  2.4269e-01,  ..., -6.7889e-01,\n",
      "           -5.7898e-01, -1.2469e+00]],\n",
      "\n",
      "         [[ 9.3392e-01,  1.6098e-01,  3.2949e-01,  ..., -1.7026e-01,\n",
      "           -6.0522e-01,  5.4642e-01],\n",
      "          [ 1.3760e+00,  6.6473e-01, -1.4035e-01,  ..., -9.4031e-02,\n",
      "           -2.4055e-01, -2.8533e-03],\n",
      "          [ 5.7931e-01,  2.1981e+00,  1.5130e+00,  ...,  1.2586e+00,\n",
      "           -9.1553e-01, -5.9304e-01],\n",
      "          ...,\n",
      "          [-3.9761e-01,  6.3133e-01,  5.4974e-01,  ...,  3.8904e-01,\n",
      "           -1.6600e+00, -1.4434e+00],\n",
      "          [-1.9706e+00,  6.3943e-01,  9.4639e-01,  ..., -1.0497e+00,\n",
      "           -2.4912e+00,  2.4691e-02],\n",
      "          [ 7.2389e-01, -3.2281e-01,  2.1177e-01,  ..., -6.0348e-01,\n",
      "           -9.6196e-01,  9.4041e-01]],\n",
      "\n",
      "         [[ 2.4312e-02, -7.0651e-01,  7.7855e-01,  ..., -1.6807e-01,\n",
      "           -3.0587e-01,  1.6635e+00],\n",
      "          [ 1.9006e+00,  1.0297e+00,  9.7183e-01,  ...,  1.6558e+00,\n",
      "            5.0725e-01,  2.6580e-01],\n",
      "          [ 1.1810e+00,  7.7411e-01,  1.3997e+00,  ...,  4.4464e-02,\n",
      "            1.4614e+00,  3.5623e-01],\n",
      "          ...,\n",
      "          [ 3.2530e-01,  2.2345e+00,  1.8286e+00,  ...,  6.5897e-01,\n",
      "            1.4874e+00,  5.3154e-01],\n",
      "          [-7.8390e-01,  1.7446e+00,  3.1448e+00,  ...,  6.2452e-01,\n",
      "            7.1587e-02,  7.4647e-01],\n",
      "          [ 1.8046e-01,  5.4822e-01,  1.8580e+00,  ..., -1.4968e-01,\n",
      "            5.3452e-01,  1.1581e+00]],\n",
      "\n",
      "         [[ 8.9811e-02,  6.5565e-02, -5.2883e-01,  ..., -1.1317e-01,\n",
      "           -1.7761e-01,  1.4173e+00],\n",
      "          [-1.0920e+00, -2.6086e-01, -1.4010e+00,  ..., -8.0427e-01,\n",
      "           -7.2595e-01,  4.1978e-01],\n",
      "          [ 6.7322e-01,  1.8720e+00,  1.7907e+00,  ..., -8.4697e-01,\n",
      "           -1.2130e+00,  1.0851e+00],\n",
      "          ...,\n",
      "          [ 5.1046e-01,  1.3918e+00,  6.7825e-01,  ...,  6.4803e-01,\n",
      "           -2.1370e-01, -9.8633e-01],\n",
      "          [-3.3402e-01, -6.5084e-02,  1.9406e-02,  ...,  1.1848e+00,\n",
      "            7.7249e-01, -1.6079e+00],\n",
      "          [ 1.0729e+00,  3.2753e-01, -6.6597e-01,  ..., -5.2663e-01,\n",
      "           -1.8795e-01,  1.2907e+00]]]], grad_fn=<ConvolutionBackward0>)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Anchor data have the size: torch.Size([4332, 4]) and the values: tensor([[-349.3803, -170.7428,  365.1697,  186.5322],\n",
      "        [-244.7368, -244.7368,  260.5263,  260.5263],\n",
      "        [-170.7428, -349.3803,  186.5322,  365.1697],\n",
      "        ...,\n",
      "        [ 234.8303,  413.4678,  949.3802,  770.7427],\n",
      "        [ 339.4737,  339.4737,  844.7368,  844.7368],\n",
      "        [ 413.4678,  234.8303,  770.7427,  949.3802]])\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# def validate(model, val_loader, device, anchors):\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     model.eval()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     val_loss = 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#             test_loss += loss.item()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#     return test_loss / len(test_loader)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 42\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# validation_loss = validate(model, validation_loader, device, anchors)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# print(f'Validation Loss: {validation_loss:.4f}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, device, anchors)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/MCs_Thesis/SOD_Thesis/models/extended_mask2former_model.py:89\u001b[0m, in \u001b[0;36mExtendedMask2Former.compute_loss\u001b[0;34m(self, predictions, targets, anchors, class_weight, bounding_box_weight, mask_weight)\u001b[0m\n\u001b[1;32m     87\u001b[0m     total_class_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_loss(pred_logits_resized, target_labels) \u001b[38;5;241m*\u001b[39m class_weight\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# total_bbox_loss += self.bounding_box_loss(pred_boxes_resized, target_boxes) * bounding_box_weight\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     total_mask_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_masks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_objects\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_masks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m mask_weight\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Combine the losses\u001b[39;00m\n\u001b[1;32m     92\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m total_class_loss \u001b[38;5;241m+\u001b[39m total_bbox_loss \u001b[38;5;241m+\u001b[39m total_mask_loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:3165\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m-> 3165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Byte"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, optimizer, device, anchors):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = model.compute_loss(outputs, targets, anchors)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "# def validate(model, val_loader, device, anchors):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in val_loader:\n",
    "#             images = torch.stack(images).to(device)\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#             outputs, _ = model(images)\n",
    "#             loss = model.compute_loss(outputs, targets, anchors)\n",
    "#             val_loss += loss.item()\n",
    "#     return val_loss / len(val_loader)\n",
    "\n",
    "# def test(model, test_loader, device, anchors):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in test_loader:\n",
    "#             images = torch.stack(images).to(device)\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#             outputs, _ = model(images)\n",
    "#             loss = model.compute_loss(outputs, targets, anchors)\n",
    "#             test_loss += loss.item()\n",
    "#     return test_loss / len(test_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, device, anchors)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # validation_loss = validate(model, validation_loader, device, anchors)\n",
    "    # print(f'Validation Loss: {validation_loss:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# # Test the model\n",
    "# test_loss = test(model, test_loader, device, anchors)\n",
    "# print(f'Test Loss: {test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
