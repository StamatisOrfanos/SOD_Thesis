{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Extended Mask2Former UAV-SOD Drone Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, json\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data_set_up import SOD_Data\n",
    "from models.extended_mask2former_model import ExtendedMask2Former\n",
    "\n",
    "\n",
    "# Import data paths\n",
    "map_path = \"src/code_map.json\"\n",
    "data_info_path = \"src/data_info/uav_data_preprocessing.json\"\n",
    "base_dir = \"data/uav_sod_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device we are going to load the model and the data\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up basic static data\n",
    "\n",
    "- Get the number of classes\n",
    "- Get the mean and standard deviation \n",
    "- Create the data paths for the [train, test, validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classes of the UAV-SOD Drone dataset\n",
    "map = open(map_path)\n",
    "data = json.load(map)\n",
    "classes = data[\"UAV_SOD_DRONE\"][\"CATEGORY_ID_TO_NAME\"]\n",
    "map.close() \n",
    "\n",
    "# The number of classes plus the background\n",
    "number_classes = len(classes) + 1\n",
    "\n",
    "\n",
    "# Load the mean and standard deviation for the train data\n",
    "map = open(data_info_path)\n",
    "data = json.load(map)\n",
    "mean = data[\"uav_data\"][\"mean\"]\n",
    "standard_deviation = data[\"uav_data\"][\"std\"]\n",
    "map.close() \n",
    "\n",
    "\n",
    "# Define train, test and validation path\n",
    "train_path = os.path.join(base_dir, \"train\")\n",
    "test_path = os.path.join(base_dir, \"test\")\n",
    "validation_path = os.path.join(base_dir, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Dataloader\n",
    "- Collate function\n",
    "- Data transformations\n",
    "- DataLoader and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transform function\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]),\n",
    "\n",
    "    \"test\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]), \n",
    "            \n",
    "    \"validation\": transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=standard_deviation)]) \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset      = SOD_Data(train_path +\"/images\", train_path + \"/annotations\", data_transform[\"train\"])\n",
    "test_dataset       = SOD_Data(test_path + \"/images\", test_path  + \"/annotations\", data_transform[\"test\"])\n",
    "validation_dataset = SOD_Data(validation_path + \"/images\", validation_path + \"/annotations\", data_transform[\"validation\"])\n",
    "\n",
    "train_loader      = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader       = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "            loss = model.compute_loss(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    }
   ],
   "source": [
    "# Model, Optimizer and Training Loop setup\n",
    "model = ExtendedMask2Former(num_classes=number_classes).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted bounding boxes are the following: tensor([[[[0.2909, 0.3229, 0.2440,  ..., 0.4697, 0.2492, 0.8057],\n",
      "          [0.6816, 0.6946, 0.6302,  ..., 0.8351, 0.5562, 0.9536],\n",
      "          [0.6461, 0.6557, 0.5852,  ..., 0.6913, 0.4583, 0.9304],\n",
      "          ...,\n",
      "          [0.6032, 0.6102, 0.7331,  ..., 0.6495, 0.6007, 0.9597],\n",
      "          [0.3870, 0.4714, 0.5422,  ..., 0.6364, 0.4467, 0.9348],\n",
      "          [0.9628, 0.8004, 0.8083,  ..., 0.8602, 0.8229, 0.9409]],\n",
      "\n",
      "         [[0.4224, 0.4650, 0.4521,  ..., 0.3527, 0.3869, 0.3364],\n",
      "          [0.4323, 0.6119, 0.5669,  ..., 0.4689, 0.5011, 0.2505],\n",
      "          [0.4625, 0.6071, 0.6905,  ..., 0.6031, 0.5138, 0.2096],\n",
      "          ...,\n",
      "          [0.3382, 0.5180, 0.7071,  ..., 0.6228, 0.5109, 0.2079],\n",
      "          [0.5390, 0.6420, 0.6864,  ..., 0.6680, 0.7315, 0.3809],\n",
      "          [0.2552, 0.5285, 0.6171,  ..., 0.5703, 0.7086, 0.2987]],\n",
      "\n",
      "         [[0.4385, 0.4278, 0.5073,  ..., 0.4690, 0.7127, 0.1003],\n",
      "          [0.6292, 0.5177, 0.5973,  ..., 0.5160, 0.7637, 0.4061],\n",
      "          [0.6036, 0.4663, 0.5867,  ..., 0.4218, 0.7318, 0.3250],\n",
      "          ...,\n",
      "          [0.8161, 0.7104, 0.7774,  ..., 0.3581, 0.7365, 0.3367],\n",
      "          [0.5839, 0.4870, 0.6401,  ..., 0.4368, 0.7496, 0.3697],\n",
      "          [0.2864, 0.4344, 0.6853,  ..., 0.5027, 0.5513, 0.7953]],\n",
      "\n",
      "         [[0.7256, 0.5614, 0.5432,  ..., 0.6457, 0.6446, 0.5676],\n",
      "          [0.4873, 0.3207, 0.2961,  ..., 0.3443, 0.4672, 0.3462],\n",
      "          [0.4552, 0.3548, 0.3669,  ..., 0.2373, 0.6129, 0.5242],\n",
      "          ...,\n",
      "          [0.6370, 0.5336, 0.3132,  ..., 0.3410, 0.4195, 0.3602],\n",
      "          [0.7901, 0.6165, 0.5893,  ..., 0.4562, 0.6199, 0.4174],\n",
      "          [0.4383, 0.3095, 0.3653,  ..., 0.2864, 0.3385, 0.6887]]]],\n",
      "       grad_fn=<SigmoidBackward0>), with shape: torch.Size([1, 4, 300, 300]) and type: <class 'torch.Tensor'>\n",
      "The actual/target bounding boxes are the following: tensor([[139,  85, 192, 121],\n",
      "        [241, 241, 257, 305],\n",
      "        [185, 109, 193, 115],\n",
      "        [ 88, 178,  94, 184],\n",
      "        [ 16, 291,  21, 295],\n",
      "        [ 55, 324,  60, 329],\n",
      "        [ 14, 346,  33, 353],\n",
      "        [180, 403, 186, 408],\n",
      "        [  1, 467,  22, 489],\n",
      "        [173, 465, 177, 471],\n",
      "        [  0, 556,  21, 573],\n",
      "        [247, 292, 253, 298]]), with shape: torch.Size([12, 4]) and type: <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# def validate(model, val_loader, device):\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     model.eval()\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     val_loss = 0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#             test_loss += loss.item()\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#     return test_loss / len(test_loader)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 45\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# validation_loss = validate(model, validation_loader, device)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# print(f'Validation Loss: {validation_loss:.4f}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/MCs_Thesis/SOD_Thesis/models/extended_mask2former_model.py:76\u001b[0m, in \u001b[0;36mExtendedMask2Former.compute_loss\u001b[0;34m(self, predictions, targets, class_weight, bounding_box_weight, mask_weight)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Reshape the bounding boxes correctly if needed\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_boxes_resized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     pred_boxes_resized \u001b[38;5;241m=\u001b[39m \u001b[43mpred_boxes_resized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Compute classification loss, bounding box loss, and mask loss for each target\u001b[39;00m\n\u001b[1;32m     80\u001b[0m total_class_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_loss(pred_logits_resized, target_labels) \u001b[38;5;241m*\u001b[39m class_weight\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = model.compute_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "# def validate(model, val_loader, device):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in val_loader:\n",
    "#             images = torch.stack(images).to(device)\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#             outputs = model(images)\n",
    "#             loss = model.compute_loss(outputs, targets)\n",
    "#             val_loss += loss.item()\n",
    "#     return val_loss / len(val_loader)\n",
    "\n",
    "# def test(model, test_loader, device):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in test_loader:\n",
    "#             images = torch.stack(images).to(device)\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#             outputs = model(images)\n",
    "#             loss = model.compute_loss(outputs, targets)\n",
    "#             test_loss += loss.item()\n",
    "#     return test_loss / len(test_loader)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # validation_loss = validate(model, validation_loader, device)\n",
    "    # print(f'Validation Loss: {validation_loss:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# # Test the model\n",
    "# test_loss = test(model, test_loader, device)\n",
    "# print(f'Test Loss: {test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
